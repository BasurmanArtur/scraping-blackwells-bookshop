{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Blackwells Bookshop\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Have you ever found yourself in this situation: you've grasped the basics of Python, you know how `for loops` work, you know what regexes are, you've solved dozens of practice problems. And now you have a nice idea of your personal project but suddenly you find out that you actually do not know Python works. How to combine all your knowledge? Where to start? \n",
    "\n",
    "In this project, I will address this problem and will try to write down my thinking and decision-making processes. It's worth noting that this project is complete, hence **it does not show the intermediate steps** I made when writing the final code. However, wherever possible, I will show the best practices I've used to write down the final code.\n",
    "\n",
    "**Note**: this project assumes that you have some basic knowledge about Python, JSON, `requests` and `BeautifulSoup` libraries.\n",
    "\n",
    "## What Are We Going to Do?\n",
    "\n",
    "In this project, we are going to reproduce a full workflow of creating a datasets: from using the web scraping, API to the data cleaning. In particular, we are going to scrape the [metadata](https://community.dataquest.io/t/the-essence-of-metadata-and-why-you-will-need-it/532478) of a collection of books from the [Blackwells Bookshop website](https://blackwells.co.uk), we will then create a dataset, clean and export it to a `csv` file. We will extract the following metadata:\n",
    "\n",
    "* ISBN\n",
    "* Title\n",
    "* Subtitle\n",
    "* Author(s) name(s)\n",
    "* Publication date\n",
    "* Current price (in euro and pounds)\n",
    "* Discount (in euro and pounds)\n",
    "* If the book is used\n",
    "* Publisher\n",
    "* Publisher's imprint\n",
    "* Country of publication\n",
    "* Edition\n",
    "* Language\n",
    "* Sales rank\n",
    "* Number of pages\n",
    "* Weight\n",
    "* Height\n",
    "* Width\n",
    "* Spine width\n",
    "\n",
    "To achieve this goal we will:\n",
    "\n",
    "* Inspect the URL to understand its structure\n",
    "* Inspect HTML of website pages to find out how to get the data we need\n",
    "* Use the `BeautifulSoup` library to scrape the content of HTML pages of the website\n",
    "* Use their API to collect the missing information\n",
    "* Use the `requests-cache` library to store the data locally\n",
    "* Create and combine two datasets\n",
    "* Handle missing data and errors\n",
    "\n",
    "Let's first of all import the necessary libraries. And here the first problem you may encounter: which packages will you need in your project? My advice is to **not concentrate too much on figuring out which libraries and functions you will use** but to import initially just the skeleton of the packages you will need. \n",
    "\n",
    "For example, you will for sure scrape the data so you will need the `BeautifulSoup` package. You will also work with dataframes so import `pandas`. In the beginning, I had no idea that the Blackwell's Bookshop website had API, so I had not think about working with the `json` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from datetime import timedelta\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for Patterns\n",
    "\n",
    "Before we dive into scraping this website we will inspect the URL. If we go to the \"Art and Design\" category on the Blackwell [website](https://blackwells.co.uk/bookshop/category/_artanddesign/) we'll see that the URL is the following:\n",
    "\n",
    "```\n",
    "https://blackwells.co.uk/bookshop/category/_artanddesign/\n",
    "```\n",
    "\n",
    "1. First, we have the URL of the main bookshop page, `https://blackwells.co.uk/`, which includes a protocol (`https://`) and the base URL, `blackwells.co.uk`.\n",
    "2. In the second part, we have three consecutive subdirectories: `bookshop`, `category` and `_artanddesign` which names are self-explanatory.\n",
    "\n",
    "What we will be interested in is the last subdirectory found in the directory `categories` because it will change based on in which category we are currently in. For example, if we go to the category *Biography* the category's name in the URL will change to `_biography`.\n",
    "\n",
    "Now follow me:\n",
    "\n",
    "* Each category contains pages with books\n",
    "* We first will have to iterate over each category\n",
    "* In each category, we have to iterate over each page\n",
    "* And on each page, we have to get the metadata of each book\n",
    "\n",
    "Seems easy, right? But how did I figure it out? It may seem easy when you're done or you have a lot of experience but for someone who just started coded (independently) it's such a pain.\n",
    "\n",
    "And here is the thing: we, humans, like categorizing things. We like patterns. So, in many cases, you will have to **search for a pattern**, something that repeats over and over again. In Python, you will use a lot of `while` and `for` loops to repeat the same code multiple times and that's how you can free your time if you have to do repetitive tasks (just write a script!).\n",
    "\n",
    "Now you have to see clearly the pattern on the Blackwell website: we have categories, and in each category, on each page we have exactly the same structure: many-many books with (almost) the same metadata.\n",
    "\n",
    "Before going on, here are a couple of useful advises:\n",
    "\n",
    "1. **Divide your scripts into functions**. In this way, you code is logically divided and you also may use the same variables (but use your common sense) to avoid mixing them up.\n",
    "2. Try to **divide your code into more manageble pieces of code**. Do not attempt to write everything in just one function. Since we are going to repeat the same task for all categories we wrote a function which returns a response from a URL with a specified category and query parameters.\n",
    "\n",
    "\n",
    "Hence, I create a function that accepts two arguments:\n",
    "\n",
    "1. `Payload` or [query parameters](https://en.wikipedia.org/wiki/Query_string) of our request to the server\n",
    "2. `Category` or a book's category\n",
    "\n",
    "This function will return the `Response` object that we will parse with the `Beautiful Soup` library to get the HTML text of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the Response object from a page in a category\n",
    "# Payload is the page number\n",
    "def blackwell_get_category_page(payload, category):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:82.0) Gecko/20100101 Firefox/82.0\"\n",
    "    }\n",
    "    url = \"https://blackwells.co.uk/bookshop/category/_{}\".format(category)\n",
    "\n",
    "    response = rq.get(url, params=payload, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move to inspecting the HTML structure.\n",
    "\n",
    "## Inspecting HTML and Finding New Patterns\n",
    "\n",
    "If we inspect an element with a book's metadata we'll see that the entire container with this data is between `<div>` tags and have CSS class `product-info` (more patterns!). To find them our you can expect the element of a book with your browser (just click RMB and select \"Inspect Element\" if you are using Firefox). Another way to find it out is to go to the page source and search for any book title/author and so on. Since many browsers adapt HTML to be human-readable it's much easier to find the information there.\n",
    "\n",
    "So, we can get each of these containers on each page and extract the data we need. However, we have some problems...\n",
    "\n",
    "In the pure HTML many authors' names and titles will be truncated (returning, for example, 'Wildlife Photographer of the Ye...'): that's because they are not written in full. In order to extract titles and authors we can use regular expressions: in fact, for each book, we have a link leading to the books page of this type: \n",
    "\n",
    "`/bookshop/product/Steps-Towards-a-Small-Theory-of-the-Visible-by-John-Berger-author/9780241472873`\n",
    "\n",
    "We can get this information directly from the link since it's a typical regular expression: we have the name of a book after `/product/` and the author's name after `-by-`.\n",
    "\n",
    "Finally, the last number is [ISBN](https://en.wikipedia.org/wiki/International_Standard_Book_Number) or International Standard Book Number, a unique code for every book in the world (and here is another pattern!). We could extract this code from the book's URL that we find in the `product-info` container. We will use it when accessing the website API and get the data to create another dataset that we will merge with the dataset we will create right now.\n",
    "\n",
    "In this section we will scrape the following metadata:\n",
    "\n",
    "* ISBN\n",
    "* Publication date\n",
    "* Current price (in euros)\n",
    "* Discount\n",
    "* Book type (such as hardback, paperback)\n",
    "* Link to the book page\n",
    "\n",
    "To extract the ISBN we will split each link by back slash (`/`) and extract the last element from the resulting list. Why so? If you look at links to books' pages you'll see the the ISBN number is always the last sequence of characters (which is another pattern).\n",
    "\n",
    "**Important note**: if you look to the right of the category page you'll see that we can apply different filters and get into subcategories of each category (which in their turn can have their subcategories). We won't scrape those books and just **concentrate our attention on the featured ones**. It's still should be enough to have a good statistical analysis.\n",
    "\n",
    "## Some Important Stuff to Prepare\n",
    "\n",
    "We will handle multiple requests to many pages in each category so we have to make sure that we **do not overwhelm the server**: to do so our script will sleep between 2 and 10 seconds (chosen randomly) after scraping the data from a page before proceeding to the next one.\n",
    "\n",
    "We will also use the `requests_cache` library that creates a local `sqlite` database where will store reponses from the server in order to accelerate things if we have to rerun our script (with a local cache it will retrieve the data from the local database, and not from the server, which is much quicker).\n",
    "\n",
    "Where did I know about the `requests_cache` library? Just read! **Read a lot of articles about the things you are going to accomplish**. I first knew about this library from a [DataQuest article](https://www.dataquest.io/blog/last-fm-api-python/) while preparing to work with APIs for the first time.\n",
    "\n",
    "## How to Iterate Through Multiple Pages?\n",
    "\n",
    "Ok, we know that metadata are inside `<div>` containers but we want to extract it from every single page in all categories. How do we work it out? \n",
    "\n",
    "Let's go the next few pages in a category and look at how the URL changes... The query parameter `offset` will increase by 48 (it makes sense since we have 48 books on every page) so that the first page in each category will have this value at 0, the second page at 48 and so on (can you the pattern?). We do not know the exact number of pages in each category (even though it should be 11 but let's assume that some categories may have less than 11 pages) so we have to make sure that we find the `product-info` CSS class and only in is this case increase the offset number and break the `while` loop otherwise.\n",
    "\n",
    "Why using a `while` loop? If we write `while True`, it will create an infinite loop that we can break when a certain condition is met (it this case, when there are no more pages to iterate over).\n",
    "\n",
    "But how do we know that there are no more pages? Just write a really big number (like 9600) to get to an $n$th page. You will see that the page still exists but there are no books on it, therefore there are no `product-info` containers.\n",
    "\n",
    "## Let's Start?\n",
    "\n",
    "Now that we resolved any doubt let's start coding. Or not?\n",
    "\n",
    "Before starting to code try to **think about**:\n",
    "\n",
    "* What your code has to do?\n",
    "* What are the inputs and what are the outputs?\n",
    "* May your code encounter any problems? If so how are you going to handle them?\n",
    "* Any possible pseudo-code? Or just a set of instructions in natural language?\n",
    "\n",
    "It may seem tedious and unproductive but it won't take you too much time and **you will be much quicker in implementing your instructions in Python**. And now we are ready to start!\n",
    "\n",
    "## Generate List of Book Categories\n",
    "\n",
    "First of all, we will extract category names to loop through in the future. If we go to [this page](https://blackwells.co.uk/bookshop/category/_top) and look at its source we'll see that links to the categories are found between `<li>` tags with the CSS class `category-list__item`.\n",
    "\n",
    "That means that we can find **all elements** between these tags and with this CSS class, extract links to category pages, find a regex (a pattern) to extract the category names. We will pass the names to the `blackwell_get_category_page`function that will return responses for each category page. \n",
    "\n",
    "The important thing to notice in the below code in the **error handling**: if the status code of a response is not 200 (which means \"OK\") the function will return the status code of the error. Here we are going to run the script only once but imagine you are going to schedule it to run every month! In this case, you should add a piece of code that would retry the execution. You can read more about this strategy [here](https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/#retry-on-failure).\n",
    "\n",
    "**Final notice**: you have to adapt your code to your plan, for example, I am going to run this code just once (or do it manually once in a while) so I do not need retry strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artanddesign', 'biography', 'business', 'childrens', 'computing']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function returns category names\n",
    "def extract_category_names():\n",
    "    url = \"https://blackwells.co.uk/bookshop/category/_top\"\n",
    "    response = rq.get(url)\n",
    "\n",
    "    # If an error occurs, return the error status code\n",
    "    if response.status_code != 200:\n",
    "        return response.text\n",
    "\n",
    "    # Otherwise extract each category element\n",
    "    else:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        cats = soup.find_all(\"li\", class_=\"category-list__item\")\n",
    "\n",
    "    # Initialize a list to hold category links\n",
    "    category_links = []\n",
    "\n",
    "    # Extract category links\n",
    "    for cat in cats:\n",
    "        category_links.append(cat.find(\"a\").get(\"href\"))\n",
    "\n",
    "    # Initialize a list to hold category names\n",
    "    category_names = []\n",
    "\n",
    "    # Search category names and extract them\n",
    "    for link in category_links:\n",
    "        category_names.append(\n",
    "            re.search(\"\\/category\\/_(\\w+)\", link).group(1)\n",
    "        )  # Use the group method to extract the first capturing\n",
    "        # group in the matched string\n",
    "\n",
    "    # Finaly return the names of categories\n",
    "    return category_names\n",
    "\n",
    "\n",
    "book_categories = extract_category_names()\n",
    "\n",
    "# Check if the returned list is correct\n",
    "book_categories[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Books' Metadata\n",
    "\n",
    "Well, now that we have a list of all categories we can write a function that will loop through all category pages and extract the data under the `product-info` class. Does it seem familiar? Head back to the section \"How to Iterate through Multiple Pages?\" where I described the workflow of what we are going to do. Why was it in the beginning? Because we have to do some preliminary research before start coding: study the HTML, the URL, answer the questions from the \"Let's Start\" section. And only then start to code!\n",
    "\n",
    "You most certainly won't be able to predict everything that's going to happen. Many ideas come when you code, or just before you start coding, so do not be discouraged if you come up with a last-minute idea or encounter a problem while coding! Just make sure you do not get overhelmed by tons of thoughts while coding, so **do good preparation**.\n",
    "\n",
    "Let's now think of what we are going to do:\n",
    "\n",
    "1. Iterate over each category\n",
    "2. Iterate over each page in that category\n",
    "3. Extract metadata of each book on that page\n",
    "\n",
    "So here are the steps we have to follow (follow them while reading the code:\n",
    "\n",
    "1. Install a local database to store the returned responses to rerun the function quickly in the future\n",
    "2. We will store the metadata in a dictionary where each key is the name of a category and each value is a list of `ResultSet` objects of the `Beautiful Soup` library\n",
    "3. We have to set the `offset_num` variable to 0. Recall that in the URL the `offset` query parameter changes by 48 when we go to the next page. Therefore, to loop through all pages we will have to update this variable at each iteration and pass it to the `blackwell_get_category_page` function as the first argument (`payload`)\n",
    "4. We have to iterate over each book category in the `book_categories` list. We will pass each name as the second argument in the `blackwell_get_category_page` function (`category`)\n",
    "5. We have to initialize a `while` loop that will `break` when there is no next page (or in other words, when there are no books on that page)\n",
    "6. We also want to keep track of what the function is doing so we can `print` the offset number (page number) and the category the function is looping through on each iteration\n",
    "7. If there is an error in our response, break the loop and print the error code\n",
    "8. Find the next page (any books on the page?), and if there are any, update the offset number (page number). We `break` the loop and set the offset number back to 0 to loop through the next category\n",
    "9. Append the `ResultSet` to the dictionary that we will `return`\n",
    "10. Finally, check the local database for the existing responses. If the response is absent, stop the function for 2-10 seconds before going on\n",
    "\n",
    "Before attempting to run/code such a big function try to test it with smaller inputs. Did it return what you wanted? If so, you can introduce `for/while` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting offset 528 for category travel\n"
     ]
    }
   ],
   "source": [
    "def extract_product_info(book_categories):\n",
    "    # Install a local database for responses, it expires after 7 days\n",
    "    requests_cache.install_cache(\n",
    "        \"blackwell_category_cache\", expire_after=timedelta(days=7)\n",
    "    )\n",
    "\n",
    "    # Dictionary with HTML of each book's metadata\n",
    "    categories_dict = {}\n",
    "\n",
    "    # Set the offset number to 0\n",
    "    offset_num = 0\n",
    "\n",
    "    # Iterate over each category name\n",
    "    for cat in book_categories:\n",
    "\n",
    "        # While there is the next page keep running the loop\n",
    "        while True:\n",
    "            payload = {\"offset\": offset_num}\n",
    "            response = blackwell_get_category_page(payload, cat)\n",
    "\n",
    "            # Print which page of which category we request\n",
    "            print(\"Requesting offset {} for category {}\".format(offset_num, cat))\n",
    "\n",
    "            # Clear output\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # If there is an error in response, break\n",
    "            if response.status_code != 200:\n",
    "                print(response.text)\n",
    "                break\n",
    "\n",
    "            # Create a BeautifulSoup object\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Check if there is a next page\n",
    "            next_page = soup.find_all(\"div\", class_=\"product-info\")\n",
    "\n",
    "            if next_page:\n",
    "                # Increase offset number to get to the next page\n",
    "                offset_num += 48\n",
    "\n",
    "            # Reset the offset number before going to the next category\n",
    "            # And break the loop if there is no next page before going to the next category\n",
    "            else:\n",
    "                offset_num = 0\n",
    "                break\n",
    "\n",
    "            # Populate the dictionary where keys are category names and values are `ResultSet` of `product-info`\n",
    "            categories_dict.setdefault(cat, []).append(next_page)\n",
    "\n",
    "            # Check if responses are in the local database, otherwise sleep 2-10 sec\n",
    "            if not getattr(response, \"from_cache\", False):\n",
    "                sleep(randint(2, 10))\n",
    "\n",
    "    return categories_dict\n",
    "\n",
    "\n",
    "product_info = extract_product_info(book_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take quite a while to run this function, so be patient.\n",
    "\n",
    "After it finished running, let's check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of each value in the dictionary is <class 'bs4.element.ResultSet'>\n",
      "\n",
      "An example of HTML of book's metadata:\n",
      "\n",
      "<div class=\"product-info\">\n",
      "<h4><a class=\"product-name\" href=\"/bookshop/product/Secrets-of-a-Devon-Wood-by-Jo-Brown-author/9781780724379\" itemprop=\"url\">Secrets of a Devon Wood</a></h4>\n",
      "<p class=\"product-author\">Jo Brown (author)</p>\n",
      "<p class=\"product-format\">\n",
      "<span>Hardback </span>\n",
      "<br/>Published <span>08 Oct 2020</span> </p>\n",
      "<div class=\"product-price\">\n",
      "<p class=\"product-price--discount u-mb-sans\">Save 2,92€</p>\n",
      "<ul class=\"list--inline\">\n",
      "<li class=\"product-price--old\"><strike>16,55€</strike></li>\n",
      "<li class=\"product-price--current\">\n",
      "                            13,63€                    </li>\n",
      "</ul>\n",
      "<span class=\"is-hidden\">In Stock</span>\n",
      "</div>\n",
      "</div>\n",
      "\n",
      "The type of each returned HTML is <class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"The type of each value in the dictionary is {}\".format(\n",
    "        type(product_info[\"artanddesign\"][0])\n",
    "    )\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\n",
    "    \"An example of HTML of book's metadata:\\n\\n{}\".format(\n",
    "        product_info[\"artanddesign\"][0][0]\n",
    "    )\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\n",
    "    \"The type of each returned HTML is {}\".format(\n",
    "        type(product_info[\"artanddesign\"][0][0])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the type of each value in the dictionary is `bs4.element.ResultSet` object where each value has `Tag` type. You can read more about this object in the [official documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "### Create the First Dataset\n",
    "\n",
    "We've successfully extracted the information about each book in each category obtaining a dictionary with categories as keys. Now the last thing left is to create the first dataset which will contain the first part of metadata. Let's study the returned dictionary `product_info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the list of the category 'artanddesign' is 11\n",
      "The length of the list of the category 'biography' is 11\n",
      "The length of the list of the category 'business' is 11\n",
      "The length of the list of the category 'childrens' is 11\n",
      "The length of the list of the category 'computing' is 11\n"
     ]
    }
   ],
   "source": [
    "# The length of dictionary values (which are lists) in the first 5 book categories\n",
    "for cat in book_categories[:5]:\n",
    "    print(\n",
    "        \"The length of the list of the category '{}' is {}\".format(\n",
    "            cat, len(product_info[cat])\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the **length corresponds to the number of pages in each category**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the ResultSet in the 'artanddesign' category on page 0 is: 48\n",
      "The length of the ResultSet in the 'artanddesign' category on page 1 is: 48\n",
      "The length of the ResultSet in the 'artanddesign' category on page 2 is: 48\n",
      "The length of the ResultSet in the 'artanddesign' category on page 3 is: 48\n",
      "The length of the ResultSet in the 'artanddesign' category on page 4 is: 48\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):\n",
    "    print(\n",
    "        \"The length of the ResultSet in the 'artanddesign' category on page {} is: {}\".format(\n",
    "            n, len(product_info[\"artanddesign\"][n])\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see on each page we have 48 books (which is equal to the length of the `ResultSet`). Recall that `ResultSet` is the HTML code of the `product-info` container which holds the book's metadata.\n",
    "\n",
    "To extract metadata from each result set we can write 3 `for loops` to iterate over each key:value pair, each `ResultSet` and each element inside it and extract the following metadata:\n",
    "\n",
    "* ISBN\n",
    "* Publication date\n",
    "* Current price (in euros)\n",
    "* Discount (in euros)\n",
    "* Book type (such as hardback, paperback, etc.)\n",
    "* Book category\n",
    "* Link to the book page\n",
    "\n",
    "We will need to use the `range` function to update the number of a page and of a book. We can assume that each category has exactly 11 pages each of which has exactly 48 books. However, the code may fail if there are less/more than 11 pages or 48 books so let's make it more robust by passing the `len` function inside the `range` function to adapt the code to the real information we have.\n",
    "\n",
    "Most of the code is straightforward because we just need to find the necessary tags and CSS classes and get the text between the tags. However, in the case of `product_format` we have two nested `span` tags so we will have to get inside these tags and extract the text. We can achieve it by finding `p` tags with the `product-format` class, then by finding **all** `span` tags which will return a list where we can access the first element (book type) and the second element (publication date).\n",
    "\n",
    "**Don't forget about the categories**: at each iteration, we will need to append them to a list to keep track of which category a book belongs to.\n",
    "\n",
    "We will also write a supplementary function `get_list_value` to handle the absence of some publication dates inside the `product-info` class (we will basically write the dictionary `get` function analogy).\n",
    "\n",
    "I discovered the necessaty of the last function when I tried to run the code and got an error. As you see **you cannot predict everything your will need to code**, so do not be discouraged if something does not work at the first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list value if it exists, NaN otherwise\n",
    "def get_list_value(lst, idx):\n",
    "    try:\n",
    "        return lst[idx].get_text()\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Creates the dataframe\n",
    "def create_first_df(product_info):\n",
    "    links = []\n",
    "    isbns = []\n",
    "    current_prices = []\n",
    "    discounts = []\n",
    "    book_types = []\n",
    "    publication_dates = []\n",
    "    categories = []\n",
    "\n",
    "    # Iterate over each category\n",
    "    for k, v in product_info.items():\n",
    "\n",
    "        # Iterate over each page and save the current ResultSet\n",
    "        for i in range(len(product_info[k])):\n",
    "            curr_result_set = v[i]\n",
    "\n",
    "            # Iterate over each book in the ResultSet and extract metadata\n",
    "            for i in range(len(product_info[k][i])):\n",
    "\n",
    "                # Extract link\n",
    "                link = curr_result_set[i].find(\"a\", class_=\"product-name\").get(\"href\")\n",
    "                links.append(link)\n",
    "\n",
    "                # Extract ISBN with a regular expression\n",
    "                isbns.append(link.split(\"/\")[-1])\n",
    "\n",
    "                # Extract current prices; if there is no price, append NaN\n",
    "                current_price = curr_result_set[i].find(\n",
    "                    \"li\", class_=\"product-price--current\"\n",
    "                )\n",
    "                if current_price:\n",
    "                    current_prices.append(current_price.get_text(strip=True))\n",
    "                else:\n",
    "                    current_prices.append(current_price)\n",
    "\n",
    "                # Extract discounts; if there is no discount, append np.nan\n",
    "                discount = curr_result_set[i].find(\n",
    "                    \"p\", class_=\"product-price--discount u-mb-sans\"\n",
    "                )\n",
    "                if discount:\n",
    "                    discounts.append(discount.get_text())\n",
    "                else:\n",
    "                    discounts.append(np.nan)\n",
    "\n",
    "                # Extract product format (book type and publication date)\n",
    "                product_format = curr_result_set[i].find(\"p\", class_=\"product-format\")\n",
    "\n",
    "                # If there is a book type and a publication date\n",
    "                if product_format:\n",
    "                    spans = product_format.find_all(\"span\")\n",
    "\n",
    "                    book_types.append(get_list_value(spans, 0))\n",
    "                    publication_dates.append(get_list_value(spans, 1))\n",
    "\n",
    "                # Otherwise append NaN\n",
    "                else:\n",
    "                    book_types.append(np.nan)\n",
    "                    publication_dates.append(np.nan)\n",
    "\n",
    "                # Append categories\n",
    "                categories.append(k)\n",
    "\n",
    "    # Create the dataframe\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"isbn\": isbns,\n",
    "            \"publication_date\": publication_dates,\n",
    "            \"price\": current_prices,\n",
    "            \"discount\": discounts,\n",
    "            \"type\": book_types,\n",
    "            \"category\": categories,\n",
    "            \"link_book_page\": links,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "first_df = create_first_df(product_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>price</th>\n",
       "      <th>discount</th>\n",
       "      <th>type</th>\n",
       "      <th>category</th>\n",
       "      <th>link_book_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781780724379</td>\n",
       "      <td>08 Oct 2020</td>\n",
       "      <td>13,63€</td>\n",
       "      <td>Save 2,92€</td>\n",
       "      <td>Hardback</td>\n",
       "      <td>artanddesign</td>\n",
       "      <td>/bookshop/product/Secrets-of-a-Devon-Wood-by-J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9780141992150</td>\n",
       "      <td>05 Nov 2020</td>\n",
       "      <td>11,03€</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>artanddesign</td>\n",
       "      <td>/bookshop/product/Poor-by-Caleb-Femi-author/97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9781250114297</td>\n",
       "      <td>06 Oct 2020</td>\n",
       "      <td>32,97€</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hardback</td>\n",
       "      <td>artanddesign</td>\n",
       "      <td>/bookshop/product/Humans-by-Brandon-Stanton-au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9781846149481</td>\n",
       "      <td>05 Nov 2020</td>\n",
       "      <td>19,11€</td>\n",
       "      <td>Save 2,97€</td>\n",
       "      <td>Hardback</td>\n",
       "      <td>artanddesign</td>\n",
       "      <td>/bookshop/product/Gamish-by-Edward-Ross-author...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9780241472859</td>\n",
       "      <td>24 Sep 2020</td>\n",
       "      <td>4,25€</td>\n",
       "      <td>Save 2,36€</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>artanddesign</td>\n",
       "      <td>/bookshop/product/The-Narrative-of-Trajans-Col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            isbn publication_date   price    discount        type  \\\n",
       "0  9781780724379      08 Oct 2020  13,63€  Save 2,92€   Hardback    \n",
       "1  9780141992150      05 Nov 2020  11,03€         NaN  Paperback    \n",
       "2  9781250114297      06 Oct 2020  32,97€         NaN   Hardback    \n",
       "3  9781846149481      05 Nov 2020  19,11€  Save 2,97€   Hardback    \n",
       "4  9780241472859      24 Sep 2020   4,25€  Save 2,36€  Paperback    \n",
       "\n",
       "       category                                     link_book_page  \n",
       "0  artanddesign  /bookshop/product/Secrets-of-a-Devon-Wood-by-J...  \n",
       "1  artanddesign  /bookshop/product/Poor-by-Caleb-Femi-author/97...  \n",
       "2  artanddesign  /bookshop/product/Humans-by-Brandon-Stanton-au...  \n",
       "3  artanddesign  /bookshop/product/Gamish-by-Edward-Ross-author...  \n",
       "4  artanddesign  /bookshop/product/The-Narrative-of-Trajans-Col...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 word of the first dataframe\n",
    "first_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Cleaning\n",
    "\n",
    "Perfect! Now that we've got the first dataframe we can move to extracting other metadata from the API but before doing that we will do some data cleaning. In particular, we will check if there are any duplicates in the `ISBN` and `link_book_page` columns since these two should be unique. It's pretty fair to suppose that the  other columns have duplicates but it does not have any impact on the data correctness (there, of course, can be duplicated publication dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate entries in the column 'isbn' is 3211\n",
      "Number of duplicate entries in the column 'link_book_page' is 3211\n"
     ]
    }
   ],
   "source": [
    "# Check ISBN and link_book_page for duplicates\n",
    "for col in [\"isbn\", \"link_book_page\"]:\n",
    "    print(\n",
    "        \"Number of duplicate entries in the column '{}' is {}\".format(\n",
    "            col, first_df[col].duplicated().sum()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more than 3000 duplicates in these two columns, and they are real duplicates (not just some random errors in ISBN) because the links have to be unique. Before we proceed we have to remove all of them to make sure that every ISBN is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate entries in the column 'isbn' is 0\n",
      "Number of duplicate entries in the column 'link_book_page' is 0\n",
      "\n",
      "Length of the dataset after removing the duplicates is 11573\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates in ISBN and link_book_page\n",
    "first_df = first_df.drop_duplicates(subset=[\"isbn\", \"link_book_page\"]).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# Check if there any duplicates\n",
    "for col in [\"isbn\", \"link_book_page\"]:\n",
    "    print(\n",
    "        \"Number of duplicate entries in the column '{}' is {}\".format(\n",
    "            col, first_df[col].duplicated().sum()\n",
    "        )\n",
    "    )\n",
    "\n",
    "print()\n",
    "\n",
    "# Print the length of the dataset after removing duplicates\n",
    "print(\n",
    "    \"Length of the dataset after removing the duplicates is {}\".format(\n",
    "        first_df.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine! We still have almost 12000 books in the dataset.\n",
    "\n",
    "Finally, let's set the ISBN as the index (we will use it to merge the datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ISBN as index\n",
    "first_df.set_index(\"isbn\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract More Metadata from the API\n",
    "\n",
    "Now that we have some general metadata for almost 12000 books (and the most important one is ISBN) we can get more specific data about each book:\n",
    "\n",
    "* Title\n",
    "* Subtitle\n",
    "* Edition\n",
    "* Author\n",
    "* Current price (in pounds)\n",
    "* Discount (in pounds)\n",
    "* If the book is used\n",
    "* Publisher\n",
    "* Publisher's imprint\n",
    "* Country of publication\n",
    "* Language\n",
    "* Sales rank\n",
    "* Number of pages\n",
    "* Weight\n",
    "* Height\n",
    "* Width\n",
    "* Spine width\n",
    "* Synopsis (long and short versions)\n",
    "* Reviews\n",
    "\n",
    "We will do it by accessing the [bookshop API](https://blackwells.co.uk/api/) which has plenty of useful functions such as getting the metadata for a book by its ISBN (and we already have them!). \n",
    "\n",
    "Before we dive into extracting the data we have to study the JSON structure. Let's first all write a function that prettifies the returning JSON file. This function will return a JSON object in a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettifies JSON file\n",
    "def prettify_json(json_obj):\n",
    "    text = json.dumps(json_obj, indent=4)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's write a function that returns a JSON response for a specified ISBN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a response for an ISBN\n",
    "def blackwell_get_book_response(isbn):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:82.0) Gecko/20100101 Firefox/82.0\"\n",
    "    }\n",
    "    url = \"http://shopfeeds.blackwell.co.uk/jsp/restful/bookdetails/{}\".format(\n",
    "        isbn\n",
    "    )  # The URL from the API documentation\n",
    "\n",
    "    response = rq.get(url, headers=headers)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed we have to study the structure of the returned JSON file. For example, we will use the book [\"Good Economics for Hard Times\"](https://blackwells.co.uk/bookshop/product/9780141986197) by Abhijit V. Banerjee and Esther Duflo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"isbn\": \"9780141986197\",\n",
      "    \"name\": \"Good Economics for Hard Times\",\n",
      "    \"author\": \"Abhijit V. Banerjee (author), Esther Duflo (author)\",\n",
      "    \"publisher\": \"Penguin Books Ltd\",\n",
      "    \"publishedDate\": \"03 Sep 2020\",\n",
      "    \"published_country\": \"United Kingdom\",\n",
      "    \"imprint\": \"Penguin Books\",\n",
      "    \"language\": \"English\",\n",
      "    \"place_of_publication\": \"UK\",\n",
      "    \"no_of_pages\": \"x, 402\",\n",
      "    \"height\": \"129\",\n",
      "    \"width\": \"196\",\n",
      "    \"depth\": \"28\",\n",
      "    \"rights_afs\": \"AD AE AF AG AI AL AM AO AQ AR AT AU AW AX AZ BA BB BE BF BG BH BI BJ BL BM BN BO BQ BR BS BV BW BY BZ CC CD CF CG CH CI CK CL CM CN CO CR CU CV CW CX CY CZ DE DJ DK DM DO DZ EC EE EG EH ER ES ET FI FJ FK FO FR GA GB GD GE GF GG GH GI GL GM GN GP GQ GR GS GT GW GY HK HM HN HR HT HU ID IE IL IM IO IQ IR IS IT JE JM JO JP KE KG KH KI KM KN KP KR KW KY KZ LA LB LC LI LR LS LT LU LV LY MA MC MD ME MF MG MK ML MM MN MO MQ MR MS MT MU MW MX MY MZ NA NC NE NF NG NI NL NO NR NU NZ OM PA PE PF PG PL PM PN PS PT PY QA RE RO RS RU RW SA SB SC SD SE SG SH SI SJ SK SL SM SN SO SR SS ST SV SX SY SZ TC TD TF TG TH TJ TK TL TM TN TO TR TT TV TW TZ UA UG UY UZ VA VC VE VG VN VU WF WS YE YT ZA ZM ZW\",\n",
      "    \"subtitle\": \"Better Answers to Our Biggest Problems\",\n",
      "    \"series_title\": \"Penguin Economics\",\n",
      "    \"has_image\": \"true\",\n",
      "    \"weight\": \"306\",\n",
      "    \"dewey\": \"330\",\n",
      "    \"bisac\": \"\",\n",
      "    \"visible\": \"1\",\n",
      "    \"dewey_edition\": \"23\",\n",
      "    \"shopstock\": \"H2:E0:T0:K1:C0:C2:Q3:R4:V0\",\n",
      "    \"audienceString\": \"\",\n",
      "    \"spine\": \"28\",\n",
      "    \"cnttype\": \"Book\",\n",
      "    \"formatString\": \"Paperback\",\n",
      "    \"short_blurb\": \"The experience of the last decade has not been kind to the image of economists: asleep at the wheel (perhaps with the foot on the gas pedal) in the run-up to the great recession, squabbling about how to get out of it, tone-deaf in discussions of the plight of Greece or the Euro area; they seem to have lost the ability to provide reliable guidance on the great problems of the day. In this ambitious, provocative book Abhijit V. Banerjee and Esther Duflo show how traditional western-centric thinking has failed to explain what is happening to people in a newly globalised world: in short Good Economics has been done badly.\",\n",
      "    \"long_blurb\": \"<p><b>FROM THE WINNERS OF THE 2019 NOBEL PRIZE IN ECONOMICS<br></b><br><b>'Wonderfully refreshing . . . A must read' Thomas Piketty </b><br><br>In this revolutionary book, prize-winning economists Abhijit V. Banerjee and Esther Duflo show how economics, when done right, can help us solve the thorniest social and political problems of our day. From immigration to inequality, slowing growth to accelerating climate change, we have the resources to address the challenges we face but we are so often blinded by ideology.<br><br>Original, provocative and urgent,<i> Good Economics for Hard Times</i> offers the new thinking that we need. It builds on cutting-edge research in economics - and years of exploring the most effective solutions to alleviate extreme poverty - to make a persuasive case for an intelligent interventionism and a society built on compassion and respect. A much-needed antidote to polarized discourse, this book shines a light to help us appreciate and understand our precariously balanced world.</p>\",\n",
      "    \"synopsisBlurb\": \"<p><b>FROM THE WINNERS OF THE 2019 NOBEL PRIZE IN ECONOMICS<br></b><br><b>'Wonderfully refreshing . . . A must read' Thomas Piketty </b><br><br>In this revolutionary book, prize-winning economists Abhijit V. Banerjee and Esther Duflo show how economics, when done right, can help us solve the thorniest social and political problems of our day. From immigration to inequality, slowing growth to accelerating climate change, we have the resources to address the challenges we face but we are so often blinded by ideology.<br><br>Original, provocative and urgent,<i> Good Economics for Hard Times</i> offers the new thinking that we need. It builds on cutting-edge research in economics - and years of exploring the most effective solutions to alleviate extreme poverty - to make a persuasive case for an intelligent interventionism and a society built on compassion and respect. A much-needed antidote to polarized discourse, this book shines a light to help us appreciate and understand our precariously balanced world.</p>\",\n",
      "    \"sJacketPath\": \"/jacket/s/9780141986197.jpg\",\n",
      "    \"mJacketPath\": \"/jacket/m/9780141986197.jpg\",\n",
      "    \"lJacketPath\": \"/jacket/l/9780141986197.jpg\",\n",
      "    \"price\": \"9.43\",\n",
      "    \"actualPrice\": \"9.43\",\n",
      "    \"originalPrice\": \"9.43\",\n",
      "    \"bigBuyButton\": \"&lt;a href=\\\"null/bookshop/shopping_cart.jsp?action=add&amp;amp;prodid=9780141986197\\\" class=\\\"bigbutton inter\\\"&gt;Add to basket&lt;/a&gt;\",\n",
      "    \"buyButton\": \"&lt;a href=\\\"null/bookshop/shopping_cart.jsp?action=add&amp;amp;prodid=9780141986197\\\" class=\\\"button inter\\\"&gt;Add to basket&lt;/a&gt;\",\n",
      "    \"dispatchDelay\": \"2\",\n",
      "    \"isSecondHand\": false,\n",
      "    \"dispatch\": \"Usually dispatched within 48 hours\",\n",
      "    \"freeQty\": 0,\n",
      "    \"actualPricePounds\": \"?9.43\",\n",
      "    \"originalPricePounds\": \"?9.43\",\n",
      "    \"pricePounds\": \"?9.43\",\n",
      "    \"totalPrice\": \"0.00\",\n",
      "    \"totalPricePounds\": \"?0.00\",\n",
      "    \"pubstatus\": 1,\n",
      "    \"gbpprice\": \"9.43\",\n",
      "    \"ipop\": \"ip\",\n",
      "    \"inStock\": true,\n",
      "    \"salesRank\": \"618\",\n",
      "    \"saveString\": \"RRP &amp;#x00A3;10.99 - &lt;strong&gt;save &amp;#x00A3;1.56&lt;/strong&gt;\",\n",
      "    \"discount\": \"1.56\",\n",
      "    \"priceBeforeDiscount\": \"&amp;#x00A3;10.99\",\n",
      "    \"priceAfterDiscount\": \"&amp;#x00A3;9.43\",\n",
      "    \"priceDifference\": \"&amp;#x00A3;1.56\",\n",
      "    \"listPrice\": \"10.99\",\n",
      "    \"code\": \"CORS\",\n",
      "    \"hasDiscount\": true,\n",
      "    \"qtyAvail\": 24,\n",
      "    \"totalQtyAvail\": 502,\n",
      "    \"blurbReview\": \"Excellent, important, disarmingly down to earth . . .  they seek to shed much-needed light upon the distortions that bad economics bring to public debates while methodically deconstructing their false assumptions.<br /><i>Observer</i><br /><br />Not all economists wear ties and think like bankers. In their wonderfully refreshing book, Banerjee and Duflo delve into impressive areas of new research questioning conventional views about issues ranging from<b> </b>trade to top income taxation and mobility, and offer their own powerful vision of how we can grapple with them. A must-read.<br /><i>Thomas Piketty, author of Capital in the Twenty-First Century</i><br /><br />Compelling, useful, relevant ... Banerjee and Duflo use extensive data to zoom out and show us a wider view of these human dynamics<br /><i>Bill Gates</i><br /><br />Excellent ... Few have grappled as energetically with the complexity of real life as Esther Duflo and Abhijit Banerjee, or got their boots as dirty in the process ... Readers will be captivated<br /><i>The Economist</i><br /><br />A canard-slaying, unconventional take on economics ... invigorating ... a treasure trove of facts and findings about the biggest economic issues of the day<br /><i>The Times</i><br /><br />A magnificent achievement, and the perfect book for our time. Banerjee and Duflo brilliantly illuminate the largest issues of the day, including immigration, trade, climate change, and inequality.<br /><i>Cass R. Sunstein, author of How Change Happens</i><br /><br />Banerjee and Duflo are masters of this terrain . . . Their book is as stimulating as it gets\",\n",
      "    \"productUrlItSelf\": \"/bookshop/product/Good-Economics-for-Hard-Times-by-Abhijit-V-Banerjee-author-Esther-Duflo-author/9780141986197\",\n",
      "    \"availableForSale\": true,\n",
      "    \"navigationList\": {\n",
      "        \"SHOPPING_CART_URL\": {\n",
      "            \"displayName\": \"SHOPPING_CART_URL\",\n",
      "            \"urlPath\": \"null/bookshop/shopping_cart.jsp?action=add&amp;prodid=9780141986197\",\n",
      "            \"hasSubNavigation\": false,\n",
      "            \"subNavigation\": [],\n",
      "            \"visible\": true\n",
      "        },\n",
      "        \"WISHLIST_BUTTON_KEY\": {\n",
      "            \"displayName\": \"Add to wishlist\",\n",
      "            \"urlPath\": \"null/bookshop/bookmark.jsp?page=9780141986197\",\n",
      "            \"hasSubNavigation\": false,\n",
      "            \"subNavigation\": [],\n",
      "            \"visible\": true\n",
      "        },\n",
      "        \"RESERVE_IN_STORE\": {\n",
      "            \"displayName\": \"Reserve at Shop\",\n",
      "            \"urlPath\": \"/bookshop/shops/shop-reserve.jsp?isbn=9780141986197&selectShop=V0&Reserve+at+shop=Reserve+at+shop\",\n",
      "            \"hasSubNavigation\": false,\n",
      "            \"subNavigation\": [],\n",
      "            \"visible\": true\n",
      "        },\n",
      "        \"BUY_BUTTON\": {\n",
      "            \"displayName\": \"Add to basket\",\n",
      "            \"urlPath\": \"\",\n",
      "            \"hasSubNavigation\": false,\n",
      "            \"subNavigation\": [],\n",
      "            \"visible\": true\n",
      "        }\n",
      "    },\n",
      "    \"productAddToBasketUrl\": \"/bookshop/shopping_cart.jsp?action=add&prodid=9780141986197\",\n",
      "    \"tax\": \"0.0\",\n",
      "    \"sku\": \"\",\n",
      "    \"skuId\": \"\",\n",
      "    \"zone\": \"\",\n",
      "    \"item_state\": \"0\",\n",
      "    \"sellerCountry\": \"\",\n",
      "    \"sellerCountryCode\": \"\",\n",
      "    \"secondHand\": false,\n",
      "    \"percentageDiscount\": false,\n",
      "    \"ljacketPath\": \"/jacket/l/9780141986197.jpg\",\n",
      "    \"mjacketPath\": \"/jacket/m/9780141986197.jpg\",\n",
      "    \"sjacketPath\": \"/jacket/s/9780141986197.jpg\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to return a long JSON-formatted object\n",
    "good_economics = blackwell_get_book_response(\"9780141986197\")\n",
    "prettify_json(good_economics.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a lot of metadata but the structure is pretty simple: it's just a dictionary of dictionaries. \n",
    "\n",
    "First of all, we will write a function that returns a dictionary where we specify the data we need. We will also have to handle some missing keys in the JSON file (like many \"edition\" fields are missed for many books) so we will specify the `np.nan` value as default if no key is found by using the dictionary function `get`. I discovered it by **trial and error**: at some point, my function returned an error because a field was absent for a book, so I had to start handling this error. I hadn't known beforehand about that but I resolved the problem once it arose.\n",
    "\n",
    "Notice that we use lists as values in the dictionary, that's because we will create dataframe from these dictionaries. It is better to use non-scalar values (like list) otherwise `pandas` will return the `ValueError: If using all scalar values, you must pass an index`. \n",
    "\n",
    "If you encounter any problem while you are writing the code, do not worry, it will happen all the time. Just head to StackOverflow, where you will find a solution in 99% of cases (which you have to understand). However, try to understand the error and read more about the it: **understanding  errors is as important as understanding solutions**. In this case, you can read more about scalar values in Python [here](https://jpt-pynotes.readthedocs.io/en/latest/scalar-types.html). After reading this you will understand that scalar values have no indexes but `pandas` requires them to create a dataframe so it throws the `ValueError` if you do not pass indexes. We've got around this problem by transforming scalar values into non-scalar ones (lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'isbn': ['9780141986197'],\n",
       " 'name': ['Good Economics for Hard Times'],\n",
       " 'subtitle': ['Better Answers to Our Biggest Problems'],\n",
       " 'edition': [nan],\n",
       " 'author': ['Abhijit V. Banerjee (author), Esther Duflo (author)'],\n",
       " 'gbpprice': ['9.43'],\n",
       " 'discount': ['1.56'],\n",
       " 'isSecondHand': [False],\n",
       " 'publisher': ['Penguin Books Ltd'],\n",
       " 'published_country': ['United Kingdom'],\n",
       " 'imprint': ['Penguin Books'],\n",
       " 'language': ['English'],\n",
       " 'no_of_pages': ['x, 402'],\n",
       " 'height': ['129'],\n",
       " 'width': ['196'],\n",
       " 'spine': ['28'],\n",
       " 'weight': ['306'],\n",
       " 'salesRank': ['618'],\n",
       " 'short_blurb': ['The experience of the last decade has not been kind to the image of economists: asleep at the wheel (perhaps with the foot on the gas pedal) in the run-up to the great recession, squabbling about how to get out of it, tone-deaf in discussions of the plight of Greece or the Euro area; they seem to have lost the ability to provide reliable guidance on the great problems of the day. In this ambitious, provocative book Abhijit V. Banerjee and Esther Duflo show how traditional western-centric thinking has failed to explain what is happening to people in a newly globalised world: in short Good Economics has been done badly.'],\n",
       " 'long_blurb': [\"<p><b>FROM THE WINNERS OF THE 2019 NOBEL PRIZE IN ECONOMICS<br></b><br><b>'Wonderfully refreshing . . . A must read' Thomas Piketty </b><br><br>In this revolutionary book, prize-winning economists Abhijit V. Banerjee and Esther Duflo show how economics, when done right, can help us solve the thorniest social and political problems of our day. From immigration to inequality, slowing growth to accelerating climate change, we have the resources to address the challenges we face but we are so often blinded by ideology.<br><br>Original, provocative and urgent,<i> Good Economics for Hard Times</i> offers the new thinking that we need. It builds on cutting-edge research in economics - and years of exploring the most effective solutions to alleviate extreme poverty - to make a persuasive case for an intelligent interventionism and a society built on compassion and respect. A much-needed antidote to polarized discourse, this book shines a light to help us appreciate and understand our precariously balanced world.</p>\"],\n",
       " 'blurbReview': ['Excellent, important, disarmingly down to earth . . .  they seek to shed much-needed light upon the distortions that bad economics bring to public debates while methodically deconstructing their false assumptions.<br /><i>Observer</i><br /><br />Not all economists wear ties and think like bankers. In their wonderfully refreshing book, Banerjee and Duflo delve into impressive areas of new research questioning conventional views about issues ranging from<b> </b>trade to top income taxation and mobility, and offer their own powerful vision of how we can grapple with them. A must-read.<br /><i>Thomas Piketty, author of Capital in the Twenty-First Century</i><br /><br />Compelling, useful, relevant ... Banerjee and Duflo use extensive data to zoom out and show us a wider view of these human dynamics<br /><i>Bill Gates</i><br /><br />Excellent ... Few have grappled as energetically with the complexity of real life as Esther Duflo and Abhijit Banerjee, or got their boots as dirty in the process ... Readers will be captivated<br /><i>The Economist</i><br /><br />A canard-slaying, unconventional take on economics ... invigorating ... a treasure trove of facts and findings about the biggest economic issues of the day<br /><i>The Times</i><br /><br />A magnificent achievement, and the perfect book for our time. Banerjee and Duflo brilliantly illuminate the largest issues of the day, including immigration, trade, climate change, and inequality.<br /><i>Cass R. Sunstein, author of How Change Happens</i><br /><br />Banerjee and Duflo are masters of this terrain . . . Their book is as stimulating as it gets']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function takes a json file as input and return a dictionary\n",
    "# where keys are metadata names and values are corresponding metadata values in lists\n",
    "def get_book_metadata(json_file) -> dict:\n",
    "\n",
    "    # Dictionary to store book metadata\n",
    "    book_metadata = {}\n",
    "\n",
    "    # Loop over necessary fields and add them to the dictionary\n",
    "    # If there is no value for this category add NaN\n",
    "    for ele in [\n",
    "        \"isbn\",\n",
    "        \"name\",\n",
    "        \"subtitle\",\n",
    "        \"edition\",\n",
    "        \"author\",\n",
    "        \"gbpprice\",\n",
    "        \"discount\",\n",
    "        \"isSecondHand\",\n",
    "        \"publisher\",\n",
    "        \"published_country\",\n",
    "        \"imprint\",\n",
    "        \"language\",\n",
    "        \"no_of_pages\",\n",
    "        \"height\",\n",
    "        \"width\",\n",
    "        \"spine\",\n",
    "        \"weight\",\n",
    "        \"salesRank\",\n",
    "        \"short_blurb\",\n",
    "        \"long_blurb\",\n",
    "        \"blurbReview\",\n",
    "    ]:\n",
    "        book_metadata[ele] = [json_file.get(ele, np.nan)]\n",
    "\n",
    "    return book_metadata\n",
    "\n",
    "\n",
    "# Print Good Economics for Hard Times metadata\n",
    "get_book_metadata(good_economics.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can do now is to loop through all ISBNs in the first dataset, call the `get_book_info` function on each response, return a dataframe and append each dataframe to the list `frames`.\n",
    "   \n",
    "Afterward, we call the function `concat` and concatenate all the dataframes from the list which will create the second dataframe containing the remaining metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting the book having ISBN 9781909911468, number 11573/11573\n"
     ]
    }
   ],
   "source": [
    "def create_second_df():\n",
    "    requests_cache.install_cache(\n",
    "        \"blackwell_books_cache\", expire_after=timedelta(days=7)\n",
    "    )\n",
    "\n",
    "    # The ISBN are indexes in the first dataframe\n",
    "    isbns = first_df.index\n",
    "\n",
    "    # Initialize the book counter to keep track while requesting book metadata\n",
    "    nr_book = 0\n",
    "\n",
    "    # Initialize a list to store responses\n",
    "    responses = []\n",
    "\n",
    "    for isbn in isbns:\n",
    "        response = blackwell_get_book_response(isbn)\n",
    "\n",
    "        # Keep track of which book we are requesting\n",
    "        print(\n",
    "            \"Requesting the book having ISBN {}, number {}/{}\".format(\n",
    "                first_df.index[nr_book], nr_book + 1, len(first_df)\n",
    "            )\n",
    "        )\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Increment the book number by 1\n",
    "        nr_book += 1\n",
    "\n",
    "        responses.append(response)\n",
    "\n",
    "        # We will make 4 requests per second if they are not saved in the local database\n",
    "        if not getattr(response, \"from_cache\", False):\n",
    "            sleep(0.25)\n",
    "\n",
    "    # Initialize a list to store dataframes\n",
    "    frames = []\n",
    "\n",
    "    # Loop through responses, transform them in dataframe and append dataframe to the list\n",
    "    for response in responses:\n",
    "        frames.append(pd.DataFrame(get_book_metadata(response.json())))\n",
    "\n",
    "    # Return concatenated frames\n",
    "    return pd.concat(frames)\n",
    "\n",
    "\n",
    "second_df = create_second_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After waiting quite a lot, we finally got the second dataframe. Let's have a quick look at it verify if it was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>name</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>edition</th>\n",
       "      <th>author</th>\n",
       "      <th>gbpprice</th>\n",
       "      <th>discount</th>\n",
       "      <th>isSecondHand</th>\n",
       "      <th>publisher</th>\n",
       "      <th>published_country</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>no_of_pages</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>spine</th>\n",
       "      <th>weight</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>short_blurb</th>\n",
       "      <th>long_blurb</th>\n",
       "      <th>blurbReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781780724379</td>\n",
       "      <td>Secrets of a Devon Wood</td>\n",
       "      <td>A Nature Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jo Brown (author)</td>\n",
       "      <td>10.40</td>\n",
       "      <td>4.59</td>\n",
       "      <td>False</td>\n",
       "      <td>Short Books</td>\n",
       "      <td>England</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>96</td>\n",
       "      <td>242</td>\n",
       "      <td>218</td>\n",
       "      <td>18</td>\n",
       "      <td>390</td>\n",
       "      <td>108</td>\n",
       "      <td>Walking one day in the woods behind her cottag...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;\"Things of such magnitude deserve respec...</td>\n",
       "      <td>&lt;b&gt;Exquisite drawings and thoughtful annotatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9780141992150</td>\n",
       "      <td>Poor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Caleb Femi (author)</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>Penguin Books Ltd</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>140</td>\n",
       "      <td>130</td>\n",
       "      <td>197</td>\n",
       "      <td>16</td>\n",
       "      <td>258</td>\n",
       "      <td>167</td>\n",
       "      <td>What is it like to grow up in a place where th...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;A &lt;i&gt;NEW STATESMAN&lt;/i&gt; AND &lt;i&gt;FINANCIAL ...</td>\n",
       "      <td>&lt;b&gt;It&amp;#39;s rare for a book of poems to repeat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781250114297</td>\n",
       "      <td>Humans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First edition</td>\n",
       "      <td>Brandon Stanton (author)</td>\n",
       "      <td>22.37</td>\n",
       "      <td>4.62</td>\n",
       "      <td>False</td>\n",
       "      <td>St. Martin's Press</td>\n",
       "      <td>New York (State)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cm</td>\n",
       "      <td>214</td>\n",
       "      <td>261</td>\n",
       "      <td>36</td>\n",
       "      <td>1628</td>\n",
       "      <td>63</td>\n",
       "      <td>\"Brandon Stanton's new book, Humans ... shows ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            isbn                     name          subtitle        edition  \\\n",
       "0  9781780724379  Secrets of a Devon Wood  A Nature Journal            NaN   \n",
       "0  9780141992150                     Poor               NaN            NaN   \n",
       "0  9781250114297                   Humans               NaN  First edition   \n",
       "\n",
       "                     author gbpprice discount  isSecondHand  \\\n",
       "0         Jo Brown (author)    10.40     4.59         False   \n",
       "0       Caleb Femi (author)     9.99     0.00         False   \n",
       "0  Brandon Stanton (author)    22.37     4.62         False   \n",
       "\n",
       "            publisher published_country  ... language no_of_pages height  \\\n",
       "0         Short Books           England  ...  English          96    242   \n",
       "0   Penguin Books Ltd    United Kingdom  ...  English         140    130   \n",
       "0  St. Martin's Press  New York (State)  ...      NaN          cm    214   \n",
       "\n",
       "  width spine weight salesRank  \\\n",
       "0   218    18    390       108   \n",
       "0   197    16    258       167   \n",
       "0   261    36   1628        63   \n",
       "\n",
       "                                         short_blurb  \\\n",
       "0  Walking one day in the woods behind her cottag...   \n",
       "0  What is it like to grow up in a place where th...   \n",
       "0  \"Brandon Stanton's new book, Humans ... shows ...   \n",
       "\n",
       "                                          long_blurb  \\\n",
       "0  <p><b>\"Things of such magnitude deserve respec...   \n",
       "0  <p><b>A <i>NEW STATESMAN</i> AND <i>FINANCIAL ...   \n",
       "0                                                NaN   \n",
       "\n",
       "                                         blurbReview  \n",
       "0  <b>Exquisite drawings and thoughtful annotatio...  \n",
       "0  <b>It&#39;s rare for a book of poems to repeat...  \n",
       "0                                                NaN  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first 3 rows\n",
    "second_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set the ISBN column as the index (we are going to join the datasets on ISBN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ISBN as index\n",
    "second_df.set_index(\"isbn\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to make sure that there are no duplicates among ISBNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of duplicates among ISBNs is 0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"The number of duplicates among ISBNs is {}\".format(\n",
    "        second_df.index.duplicated().sum()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems okay, so the next step is to join the dataframe to create the final dataset (that we will clean a little bit). We will use the inner join and **the length of the final dataset should be equal to the lengths of the datasets we join**.\n",
    "\n",
    "### Joining, Cleaning and Exporting Datasets\n",
    "\n",
    "We will use the `join` method since we are going to join the datasets by indexes (ISBNs), although we could use the `merge` method that can combine datasets by a specified column. Note that we use the `lsuffix` and `rsuffix` parameters to distinguish between two discounts (in euros in the first dataset, and in pounds in the second dataset). That said, **consult the documentation** to find out many interesting parameters you can use in your project.\n",
    "\n",
    "We will also export the dataframe to a `csv` file before attempting the cleaning to provide users with the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>price</th>\n",
       "      <th>discount_euro</th>\n",
       "      <th>type</th>\n",
       "      <th>category</th>\n",
       "      <th>link_book_page</th>\n",
       "      <th>name</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>edition</th>\n",
       "      <th>author</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>no_of_pages</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>spine</th>\n",
       "      <th>weight</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>short_blurb</th>\n",
       "      <th>long_blurb</th>\n",
       "      <th>blurbReview</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isbn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9781780724379</th>\n",
       "      <td>08 Oct 2020</td>\n",
       "      <td>13,63€</td>\n",
       "      <td>Save 2,92€</td>\n",
       "      <td>Hardback</td>\n",
       "      <td>artanddesign</td>\n",
       "      <td>/bookshop/product/Secrets-of-a-Devon-Wood-by-J...</td>\n",
       "      <td>Secrets of a Devon Wood</td>\n",
       "      <td>A Nature Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jo Brown (author)</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>96</td>\n",
       "      <td>242</td>\n",
       "      <td>218</td>\n",
       "      <td>18</td>\n",
       "      <td>390</td>\n",
       "      <td>108</td>\n",
       "      <td>Walking one day in the woods behind her cottag...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;\"Things of such magnitude deserve respec...</td>\n",
       "      <td>&lt;b&gt;Exquisite drawings and thoughtful annotatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9780141992150</th>\n",
       "      <td>05 Nov 2020</td>\n",
       "      <td>11,03€</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>artanddesign</td>\n",
       "      <td>/bookshop/product/Poor-by-Caleb-Femi-author/97...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Caleb Femi (author)</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>140</td>\n",
       "      <td>130</td>\n",
       "      <td>197</td>\n",
       "      <td>16</td>\n",
       "      <td>258</td>\n",
       "      <td>167</td>\n",
       "      <td>What is it like to grow up in a place where th...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;A &lt;i&gt;NEW STATESMAN&lt;/i&gt; AND &lt;i&gt;FINANCIAL ...</td>\n",
       "      <td>&lt;b&gt;It&amp;#39;s rare for a book of poems to repeat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9781250114297</th>\n",
       "      <td>06 Oct 2020</td>\n",
       "      <td>32,97€</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hardback</td>\n",
       "      <td>artanddesign</td>\n",
       "      <td>/bookshop/product/Humans-by-Brandon-Stanton-au...</td>\n",
       "      <td>Humans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First edition</td>\n",
       "      <td>Brandon Stanton (author)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cm</td>\n",
       "      <td>214</td>\n",
       "      <td>261</td>\n",
       "      <td>36</td>\n",
       "      <td>1628</td>\n",
       "      <td>63</td>\n",
       "      <td>\"Brandon Stanton's new book, Humans ... shows ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              publication_date   price discount_euro        type  \\\n",
       "isbn                                                               \n",
       "9781780724379      08 Oct 2020  13,63€    Save 2,92€   Hardback    \n",
       "9780141992150      05 Nov 2020  11,03€           NaN  Paperback    \n",
       "9781250114297      06 Oct 2020  32,97€           NaN   Hardback    \n",
       "\n",
       "                   category  \\\n",
       "isbn                          \n",
       "9781780724379  artanddesign   \n",
       "9780141992150  artanddesign   \n",
       "9781250114297  artanddesign   \n",
       "\n",
       "                                                  link_book_page  \\\n",
       "isbn                                                               \n",
       "9781780724379  /bookshop/product/Secrets-of-a-Devon-Wood-by-J...   \n",
       "9780141992150  /bookshop/product/Poor-by-Caleb-Femi-author/97...   \n",
       "9781250114297  /bookshop/product/Humans-by-Brandon-Stanton-au...   \n",
       "\n",
       "                                  name          subtitle        edition  \\\n",
       "isbn                                                                      \n",
       "9781780724379  Secrets of a Devon Wood  A Nature Journal            NaN   \n",
       "9780141992150                     Poor               NaN            NaN   \n",
       "9781250114297                   Humans               NaN  First edition   \n",
       "\n",
       "                                 author  ... language no_of_pages  height  \\\n",
       "isbn                                     ...                                \n",
       "9781780724379         Jo Brown (author)  ...  English          96     242   \n",
       "9780141992150       Caleb Femi (author)  ...  English         140     130   \n",
       "9781250114297  Brandon Stanton (author)  ...      NaN          cm     214   \n",
       "\n",
       "              width spine weight salesRank  \\\n",
       "isbn                                         \n",
       "9781780724379   218    18    390       108   \n",
       "9780141992150   197    16    258       167   \n",
       "9781250114297   261    36   1628        63   \n",
       "\n",
       "                                                     short_blurb  \\\n",
       "isbn                                                               \n",
       "9781780724379  Walking one day in the woods behind her cottag...   \n",
       "9780141992150  What is it like to grow up in a place where th...   \n",
       "9781250114297  \"Brandon Stanton's new book, Humans ... shows ...   \n",
       "\n",
       "                                                      long_blurb  \\\n",
       "isbn                                                               \n",
       "9781780724379  <p><b>\"Things of such magnitude deserve respec...   \n",
       "9780141992150  <p><b>A <i>NEW STATESMAN</i> AND <i>FINANCIAL ...   \n",
       "9781250114297                                                NaN   \n",
       "\n",
       "                                                     blurbReview  \n",
       "isbn                                                              \n",
       "9781780724379  <b>Exquisite drawings and thoughtful annotatio...  \n",
       "9780141992150  <b>It&#39;s rare for a book of poems to repeat...  \n",
       "9781250114297                                                NaN  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join datasets by ISBN\n",
    "final_df = first_df.join(second_df, lsuffix=\"_euro\", rsuffix=\"_gbp\")\n",
    "\n",
    "# Export the final dataframe to a csv file\n",
    "final_df.to_csv(\"blackwell_shop_raw.csv\")\n",
    "\n",
    "# Show the first three rows of the final dataset\n",
    "final_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to check two things:\n",
    "\n",
    "* That the number of rows in the final dataset is equal to the number of rows of the first two datasets\n",
    "* That the total number of columns in the final dataset is the sum of the columns of the first and the second datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the length of the final dataset equal to the lengths of the first two dataset? True\n",
      "Is the total number of columns in the final dataset the sum of the columns of the first and the second datasets? True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Is the length of the final dataset equal to the lengths of the first two dataset? {}\".format(\n",
    "        first_df.shape[0] == second_df.shape[0] == final_df.shape[0]\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Is the total number of columns in the final dataset the sum of the columns of the first and the second datasets? {}\".format(\n",
    "        first_df.shape[1] + second_df.shape[1] == final_df.shape[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine! Our dataframe was created correctly.\n",
    "\n",
    "The next step is to clean a bit the dataset before sharing it. Here is what we can do:\n",
    "\n",
    "1. Transform `object` types to `int/float` types where possible (like discounts, number of pages, etc.)\n",
    "2. Rename the `price` column to `euro_price` and the `gbpprice` column to `gbp_price`\n",
    "\n",
    "We, of course, can rename categories to be more readable or transform publication dates in `datetime` objects but let's leave to whoever is going to do data analysis. We can also remove HTML tags from the last three columns but it may be dangerous so we should be very careful so let's leave this decision to a data analyst (HTML tags should not interfere with somehting like sentiment analysis).\n",
    "\n",
    "Let's start by selecting which columns we want to transform to integers of float numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['publication_date', 'price', 'discount_euro', 'type', 'category',\n",
       "       'link_book_page', 'name', 'subtitle', 'edition', 'author', 'gbpprice',\n",
       "       'discount_gbp', 'isSecondHand', 'publisher', 'published_country',\n",
       "       'imprint', 'language', 'no_of_pages', 'height', 'width', 'spine',\n",
       "       'weight', 'salesRank', 'short_blurb', 'long_blurb', 'blurbReview'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show columns of the final dataset\n",
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that numeric columns are:\n",
    "\n",
    "* Price and discount (in euros and pounds)\n",
    "* Number of pages\n",
    "* Height\n",
    "* Width\n",
    "* Spine\n",
    "* Weight\n",
    "* Sales rank\n",
    "\n",
    "In case of prices and discounts we will need to do some transformations so let's leave them for now. Also the `no_of_pages` column contains the data which is not easy to understand, like \"269 , 16 unnumbered  of plates\", so we won't do anything about it.\n",
    "\n",
    "We will use the `pandas` `to_numeric` function which robustly converts strings to numbers. However, before we transform the dataset to a `csv` file, it's important to make sure that we do not introduce missing values. Let's compute the number of null values before and after the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in each column before the transformation is \n",
      "height        4\n",
      "width         5\n",
      "spine        82\n",
      "weight        0\n",
      "salesRank     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# List of columns to be transformed to numbers\n",
    "cols_to_numeric = [\"height\", \"width\", \"spine\", \"weight\", \"salesRank\"]\n",
    "\n",
    "# Print out number of null values before transformation\n",
    "print(\n",
    "    \"The number of null values in each column before the transformation is \\n{}\".format(\n",
    "        final_df[cols_to_numeric].isnull().sum()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform values in selected columns to numbers\n",
    "for col in cols_to_numeric:\n",
    "    final_df[col] = pd.to_numeric(final_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in each column after the transformation is \n",
      "height          4\n",
      "width           5\n",
      "spine          82\n",
      "weight          0\n",
      "salesRank    2109\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"The number of null values in each column after the transformation is \\n{}\".format(\n",
    "        final_df[cols_to_numeric].isnull().sum()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've introduced 2109 missing values to the `salesRank` column! Let's revert to the original dataframe and study this column. We may assume that this column has some non-alphanumeric characters (like punctuation) which causes the `pd.to_numeric` function to introduce null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isbn\n",
       "9781913107161     \n",
       "9780300253801     \n",
       "9780520304840     \n",
       "9780500971062     \n",
       "9781426221422     \n",
       "                ..\n",
       "9780241959909     \n",
       "9780140455076     \n",
       "9781840182316     \n",
       "9781912177059     \n",
       "9781909911468     \n",
       "Name: salesRank, Length: 2109, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revert to the original dataframe\n",
    "final_df = first_df.join(second_df, lsuffix=\"_euro\", rsuffix=\"_gbp\")\n",
    "\n",
    "# Check if the salesRank column has non-alphabetic characters\n",
    "final_df[~final_df[\"salesRank\"].str.isalnum()][\"salesRank\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2701 non-alphanumeric characters many of which seem to be blank spaces. Let's check this hypothesis by finding all space characters and printing out their amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of values with blank spaces in the 'salesRank' columns is 2109\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"The number of values with blank spaces in the 'salesRank' columns is {}\".format(\n",
    "        final_df[final_df[\"salesRank\"].str.contains(r\"^\\s*$\", regex=True)].shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers are the same so we can conclude that the `pd.to_numeric` function replaces them with null values and we do not lose any information. Now we can proceed with this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of transformed columns:\n",
      "\n",
      "height       float64\n",
      "width        float64\n",
      "spine        float64\n",
      "weight         int64\n",
      "salesRank    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Transform values in selected columns to numbers\n",
    "for col in cols_to_numeric:\n",
    "    final_df[col] = pd.to_numeric(final_df[col])\n",
    "\n",
    "# Print types of transformed columns\n",
    "print(\"Types of transformed columns:\")\n",
    "print()\n",
    "print(final_df[cols_to_numeric].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now the values are either of the `float64` or `int64` type. We can now go on with cleaning the price and discounts columns. First of all, let's rename the price columns to distinguish between prices in euros and pounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename price columns\n",
    "final_df.rename({\"price\": \"euro_price\", \"gbpprice\": \"gbp_price\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the `euro_price` column using the `sample` function from `pandas` which returns a random sample from this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isbn\n",
       "9780552779838     11,02€\n",
       "9781506717661     16,54€\n",
       "9789811531057    135,88€\n",
       "9781138931367     36,65€\n",
       "9781529034554      7,85€\n",
       "9781787702073      8,74€\n",
       "9780571314430     28,71€\n",
       "9780198853923     36,22€\n",
       "9781912891245     14,91€\n",
       "9780751566659      7,59€\n",
       "Name: euro_price, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a sample of 10 euro prices\n",
    "final_df[\"euro_price\"].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first glance, we have to remove the euro symbol and change commas to dots to be able to transform the prices to numeric values. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace commas with dots and remove euro symbol\n",
    "final_df[\"euro_price\"] = (\n",
    "    final_df[\"euro_price\"].str.replace(\",\", \".\").str.replace(\"€\", \"\")\n",
    ")\n",
    "\n",
    "# Transform euro prices to numeric values\n",
    "final_df[\"euro_price\"] = pd.to_numeric(final_df[\"euro_price\"])\n",
    "\n",
    "# Print out the type of the euro_price column\n",
    "final_df[\"euro_price\"].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine! The type of the `euro_price` columns is `float64`. As an exercise check if we introduce any missing value to the column during the transformation.\n",
    "\n",
    "Let's go on with the `gbp_price` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isbn\n",
       "9781108826716     7.39\n",
       "9780262044127    30.33\n",
       "9780262044080    13.99\n",
       "9781471172854    15.28\n",
       "9781621138082    16.32\n",
       "9780521797948    19.22\n",
       "9781784724269     9.25\n",
       "9781472278401    10.40\n",
       "9780201882957    23.71\n",
       "9781138850903    38.45\n",
       "Name: gbp_price, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[\"gbp_price\"].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, here we can try to directly transform the values to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform pound prices to numeric values\n",
    "final_df[\"gbp_price\"] = pd.to_numeric(final_df[\"gbp_price\"])\n",
    "\n",
    "# Print out the type of the gbp_price column\n",
    "final_df[\"gbp_price\"].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also this column is of `float64` type. Do not forget to check if there are any introduced missing values.\n",
    "\n",
    "Now let's go on with discounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isbn\n",
       "9780241343265           NaN\n",
       "9780751573954    Save 1,27€\n",
       "9781913097073    Save 1,57€\n",
       "9780199646593    Save 2,65€\n",
       "9781108480918    Save 3,78€\n",
       "9780141980546    Save 1,55€\n",
       "9780300254778           NaN\n",
       "9781138953949    Save 1,96€\n",
       "9780140021967    Save 1,60€\n",
       "9781138808089    Save 0,04€\n",
       "Name: discount_euro, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[\"discount_euro\"].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to remove the `Save` string, the euro symbol and replace commas with dots before transforming the discounts to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3050"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[\"discount_euro\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Save and €, replace commas with dots\n",
    "final_df[\"discount_euro\"] = (\n",
    "    final_df[\"discount_euro\"]\n",
    "    .str.replace(\"Save \", \"\")\n",
    "    .str.replace(\",\", \".\")\n",
    "    .str.replace(\"€\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to transform the values we'll get the `ValueError` because some discounts are in percentages (like 30%). We can impute the discounts by subtracting the discounted price from the full price. Note that prices in euros in the dataframe are already discounted.\n",
    "\n",
    "We can subset the data and then merge it with the original dataframe. It will allow us to concentrate only on a specific chunk of information and do not get lost while performing math operations.\n",
    "\n",
    "The formula to compute the discounts is $discount\\_euro$ = $(euro\\_price * (1 + discount\\_euro\\_in\\_\\%)) - euro\\_price$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>euro_price</th>\n",
       "      <th>discount_euro</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isbn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9780198724643</th>\n",
       "      <td>4.62</td>\n",
       "      <td>1.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9780198786191</th>\n",
       "      <td>7.72</td>\n",
       "      <td>2.316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               euro_price  discount_euro\n",
       "isbn                                    \n",
       "9780198724643        4.62          1.386\n",
       "9780198786191        7.72          2.316"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset of the data with discounts in percents\n",
    "subset = final_df.loc[final_df[\"discount_euro\"].str.contains(\"%\", na=False)].copy()\n",
    "\n",
    "# Remove percent symbols\n",
    "subset[\"discount_euro\"] = subset[\"discount_euro\"].str.replace(\"%\", \"\")\n",
    "\n",
    "# Transform discounts in decimal numbers\n",
    "subset[\"discount_euro\"] = pd.to_numeric(subset[\"discount_euro\"]) / 100\n",
    "\n",
    "# Apply the formula to compute discounts in euros\n",
    "subset[\"discount_euro\"] = (\n",
    "    subset[\"euro_price\"] * (1 + subset[\"discount_euro\"])\n",
    ") - subset[\"euro_price\"]\n",
    "\n",
    "# Check if everything is OK\n",
    "subset[[\"euro_price\", \"discount_euro\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is fine so we can combine this subset with the original dataframe. We will select the indexes of the subset and just change the discounts corresponding to those ISBNs in the original dataset (so we did not even use the `join` method that would require us to specify suffixes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change euro discounts from percents to numbers\n",
    "final_df.loc[subset.index, \"discount_euro\"] = subset[\"discount_euro\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last transformation left is of the discounts in pounds. Let's study the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isbn\n",
       "9781780724379     4.59\n",
       "9780141992150     0.00\n",
       "9781250114297     4.62\n",
       "9781846149481     6.01\n",
       "9780241472859     2.31\n",
       "                 ...  \n",
       "9781250038821     3.00\n",
       "9781910240755     3.00\n",
       "9781910702390     6.27\n",
       "9781912177059     2.83\n",
       "9781909911468    10.68\n",
       "Name: discount_gbp, Length: 11573, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out discounts in pounds\n",
    "final_df[\"discount_gbp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be straightforward: just directly transform them to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform discounts in pounds to numbers\n",
    "final_df[\"discount_gbp\"] = pd.to_numeric(final_df[\"discount_gbp\"])\n",
    "\n",
    "# Check the type\n",
    "final_df[\"discount_gbp\"].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine! All the numbers are floats now. Do not forget to check if there are any introduced missing value as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>euro_price</th>\n",
       "      <th>discount_euro</th>\n",
       "      <th>type</th>\n",
       "      <th>category</th>\n",
       "      <th>link_book_page</th>\n",
       "      <th>name</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>edition</th>\n",
       "      <th>author</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>no_of_pages</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>spine</th>\n",
       "      <th>weight</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>short_blurb</th>\n",
       "      <th>long_blurb</th>\n",
       "      <th>blurbReview</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isbn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9781780724379</th>\n",
       "      <td>08 Oct 2020</td>\n",
       "      <td>13.63</td>\n",
       "      <td>2.92</td>\n",
       "      <td>Hardback</td>\n",
       "      <td>artanddesign</td>\n",
       "      <td>/bookshop/product/Secrets-of-a-Devon-Wood-by-J...</td>\n",
       "      <td>Secrets of a Devon Wood</td>\n",
       "      <td>A Nature Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jo Brown (author)</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>96</td>\n",
       "      <td>242.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>390</td>\n",
       "      <td>108.0</td>\n",
       "      <td>Walking one day in the woods behind her cottag...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;\"Things of such magnitude deserve respec...</td>\n",
       "      <td>&lt;b&gt;Exquisite drawings and thoughtful annotatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9780141992150</th>\n",
       "      <td>05 Nov 2020</td>\n",
       "      <td>11.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paperback</td>\n",
       "      <td>artanddesign</td>\n",
       "      <td>/bookshop/product/Poor-by-Caleb-Femi-author/97...</td>\n",
       "      <td>Poor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Caleb Femi (author)</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>140</td>\n",
       "      <td>130.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>258</td>\n",
       "      <td>167.0</td>\n",
       "      <td>What is it like to grow up in a place where th...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;A &lt;i&gt;NEW STATESMAN&lt;/i&gt; AND &lt;i&gt;FINANCIAL ...</td>\n",
       "      <td>&lt;b&gt;It&amp;#39;s rare for a book of poems to repeat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              publication_date  euro_price discount_euro        type  \\\n",
       "isbn                                                                   \n",
       "9781780724379      08 Oct 2020       13.63          2.92   Hardback    \n",
       "9780141992150      05 Nov 2020       11.03           NaN  Paperback    \n",
       "\n",
       "                   category  \\\n",
       "isbn                          \n",
       "9781780724379  artanddesign   \n",
       "9780141992150  artanddesign   \n",
       "\n",
       "                                                  link_book_page  \\\n",
       "isbn                                                               \n",
       "9781780724379  /bookshop/product/Secrets-of-a-Devon-Wood-by-J...   \n",
       "9780141992150  /bookshop/product/Poor-by-Caleb-Femi-author/97...   \n",
       "\n",
       "                                  name          subtitle edition  \\\n",
       "isbn                                                               \n",
       "9781780724379  Secrets of a Devon Wood  A Nature Journal     NaN   \n",
       "9780141992150                     Poor               NaN     NaN   \n",
       "\n",
       "                            author  ...  language  no_of_pages  height  width  \\\n",
       "isbn                                ...                                         \n",
       "9781780724379    Jo Brown (author)  ...   English           96   242.0  218.0   \n",
       "9780141992150  Caleb Femi (author)  ...   English          140   130.0  197.0   \n",
       "\n",
       "              spine weight salesRank  \\\n",
       "isbn                                   \n",
       "9781780724379  18.0    390     108.0   \n",
       "9780141992150  16.0    258     167.0   \n",
       "\n",
       "                                                     short_blurb  \\\n",
       "isbn                                                               \n",
       "9781780724379  Walking one day in the woods behind her cottag...   \n",
       "9780141992150  What is it like to grow up in a place where th...   \n",
       "\n",
       "                                                      long_blurb  \\\n",
       "isbn                                                               \n",
       "9781780724379  <p><b>\"Things of such magnitude deserve respec...   \n",
       "9780141992150  <p><b>A <i>NEW STATESMAN</i> AND <i>FINANCIAL ...   \n",
       "\n",
       "                                                     blurbReview  \n",
       "isbn                                                              \n",
       "9781780724379  <b>Exquisite drawings and thoughtful annotatio...  \n",
       "9780141992150  <b>It&#39;s rare for a book of poems to repeat...  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we export the cleaned dataframe to a `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "final_df.to_csv(\"blackwell_shop_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Let's wrap up what we did:\n",
    "\n",
    "* Inspected the URL to understand its structure\n",
    "* Inspected HTML of website pages to find out how to get the data we need\n",
    "* Used the `BeautifulSoup` library to scrape the content of HTML pages of the website\n",
    "* Used their API to collect the missing information\n",
    "* Used the `requests-cache` library to store the data locally\n",
    "* Created and combined two datasets\n",
    "* Handled missing data and errors\n",
    "\n",
    "In this project, I addressed the problem of not being able to apply your knowledge in real-world situations and I hope I was able to gave you some useful tips on how to organize your thinking process when working independently. Let's summarise the main points:\n",
    "\n",
    "1. Do not concentrate too much on figuring out which libraries and functions you will use, just import the ones you will need for sure\n",
    "2. Look for patterns\n",
    "3. Divide your code in easily manageable functions\n",
    "4. Do a good preparation before start coding, it will save your time\n",
    "5. Consult the documentation\n",
    "\n",
    "Good luck in your data science journey and happy coding!\n",
    "\n",
    "## Bonus: Inspiration\n",
    "\n",
    "Some of the interesting tasks to perform on this dataset:\n",
    "\n",
    "1. Analyze how prices change from category to category \n",
    "2. Predict prices by analyzing different parameters\n",
    "3. Do a sentiment analysis of blurbs and reviews\n",
    "4. Combine the dataset with [other data sources](https://www.kaggle.com/jealousleopard/goodreadsbooks) to look for new insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
